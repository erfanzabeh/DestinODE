{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hcekcing the compute aspec and epoch run time per second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions: 8 | Neurons kept: 16/305 | Example lap shape: torch.Size([668, 16])\n",
      "‣ deriving Day‑0 statistics for W0 and b_fixed …\n",
      "Epoch 000 | total 3.649e+03 | rec+dec 3.429e+03\n",
      "Epoch 005 | total 2.192e+03 | rec+dec 1.854e+03\n",
      "Epoch 010 | total 8.811e+02 | rec+dec 9.773e+02\n",
      "Epoch 015 | total 5.657e+02 | rec+dec 8.556e+02\n",
      "Epoch 020 | total 5.476e+02 | rec+dec 6.504e+02\n",
      "Epoch 025 | total 5.438e+02 | rec+dec 7.545e+02\n",
      "Epoch 030 | total 5.410e+02 | rec+dec 3.101e+02\n",
      "Epoch 035 | total 5.390e+02 | rec+dec 6.378e+02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 172\u001b[0m\n\u001b[1;32m    170\u001b[0m     smooth  \u001b[38;5;241m=\u001b[39m (Ws[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m-\u001b[39mWs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    171\u001b[0m     loss \u001b[38;5;241m=\u001b[39m rec_dec \u001b[38;5;241m+\u001b[39m LAMBDA_SMOOTH\u001b[38;5;241m*\u001b[39msmooth\n\u001b[0;32m--> 172\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad(); loss\u001b[38;5;241m.\u001b[39mbackward(); opt\u001b[38;5;241m.\u001b[39mstep(); total\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ep\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| rec+dec \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrec_dec\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "#  DESTINODE with neuron & lap subsampling  (variable‑K, u‑decoder)\n",
    "# ===============================================================\n",
    "import numpy as np, torch, matplotlib.pyplot as plt, torch.nn as nn, random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "# ------------------- 0. KNOBS -------------------\n",
    "KEEP_NEURON_FRAC = 0.05          # 0–1   (20 % of neurons)\n",
    "KEEP_LAP_FRAC    = 0.4          # 0–1   (50 % of laps per session)\n",
    "RNG_SEED         = 42            # for reproducibility\n",
    "torch.manual_seed(RNG_SEED); np.random.seed(RNG_SEED); random.seed(RNG_SEED)\n",
    "\n",
    "# ------------------ 1. SETTINGS ------------------\n",
    "SUBSET_FILE   = \"destin_debug_subset.npz\"\n",
    "rank          = 3\n",
    "train_days    = [0, 1, 2, 3]\n",
    "test_day      = 6\n",
    "epochs        = 10\n",
    "LAMBDA_SMOOTH = 5e-2\n",
    "device        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ------------------ 2. LOAD & SUBSAMPLE ----------\n",
    "data = np.load(SUBSET_FILE, allow_pickle=True)\n",
    "sess_ids = sorted({int(k.split('_')[1]) for k in data if k.startswith('dff_')})\n",
    "\n",
    "# 2a. decide once‑and‑for‑all neuron mask\n",
    "N_full = data['dff_0'].shape[0]\n",
    "n_keep = int(np.ceil(N_full * KEEP_NEURON_FRAC))\n",
    "keep_neurons = np.random.choice(N_full, n_keep, replace=False)\n",
    "keep_neurons.sort()                                      # keep order\n",
    "\n",
    "to_tensor = lambda x: torch.tensor(x, dtype=torch.float32)\n",
    "x_sess = []; v_sess = []; u_sess = []\n",
    "for t in sess_ids:\n",
    "    dff_full = data[f'dff_{t}'][keep_neurons]             # [N_keep, K_vr]\n",
    "    dff_full = to_tensor(dff_full.T)                      # [K_vr, N_keep]\n",
    "    vel_full = to_tensor(data[f'vel_{t}'])\n",
    "    pos_full = to_tensor(data[f'pos_{t}'])\n",
    "    pos_full = (pos_full - pos_full.mean()) / pos_full.std()\n",
    "\n",
    "    lap_idx  = data[f'lap_idx_{t}']\n",
    "\n",
    "    # 2b. optional lap subsampling\n",
    "    all_laps = np.unique(lap_idx)\n",
    "    n_laps_keep = int(np.ceil(len(all_laps) * KEEP_LAP_FRAC))\n",
    "    keep_laps = np.random.choice(all_laps, n_laps_keep, replace=False)\n",
    "\n",
    "    laps_x = []; laps_v = []; laps_u = []\n",
    "    for s in sorted(keep_laps):\n",
    "        mask = lap_idx == s\n",
    "        if not mask.any(): continue\n",
    "        laps_x.append(dff_full[mask])\n",
    "        laps_v.append(vel_full[mask])\n",
    "        laps_u.append(pos_full[mask])\n",
    "\n",
    "    x_sess.append(laps_x); v_sess.append(laps_v); u_sess.append(laps_u)\n",
    "\n",
    "T, N = len(x_sess), len(keep_neurons)\n",
    "print(f\"Sessions: {T} | Neurons kept: {N}/{N_full} \"\n",
    "      f\"| Example lap shape: {x_sess[0][0].shape}\")\n",
    "\n",
    "# ------------------ 3. DATASET -------------------\n",
    "class RaggedSplitDS(Dataset):\n",
    "    def __init__(self, x, v, u, days):\n",
    "        self.x,self.v,self.u = x,v,u\n",
    "        self.map = [(t,s) for t in days for s in range(len(x[t]))]\n",
    "    def __len__(self): return len(self.map)\n",
    "    def __getitem__(self, idx):\n",
    "        t,s = self.map[idx]\n",
    "        return self.x[t][s], self.v[t][s], self.u[t][s], t\n",
    "\n",
    "loader = DataLoader(RaggedSplitDS(x_sess,v_sess,u_sess,train_days),\n",
    "                    batch_size=1, shuffle=True, collate_fn=lambda b:b)\n",
    "\n",
    "# ------------------ 4. MODEL (unchanged except N) -----------\n",
    "W0 = 0.05*torch.randn(N,N)\n",
    "U,_,Vt = torch.linalg.svd(W0)\n",
    "U = U[:,:rank]; V = Vt[:rank,:].T\n",
    "B = 0.05*torch.randn(N,1); R = 0.05*torch.randn(1,N)\n",
    "\n",
    "# ===============================================================\n",
    "#  DAY‑0‑DRIVEN INITIALISATION  (insert before “# ------------------ 4. MODEL ---------------------”)\n",
    "# ===============================================================\n",
    "print(\"‣ deriving Day‑0 statistics for W0 and b_fixed …\")\n",
    "\n",
    "# -------- gather all frames from session‑0 after subsampling --------\n",
    "X0 = torch.cat(x_sess[0], dim=0)           # [K_total_day0 , N]\n",
    "X0 = X0.float()                            # ensure fp32\n",
    "\n",
    "# -------- 1) activation bias  b_fixed -------------------------------\n",
    "avg_rate = X0.mean(0)                      # mean across time\n",
    "b_init   = -(avg_rate - avg_rate.mean())   # anti‑correlated w/ firing\n",
    "b_init   = (0.5 / b_init.abs().max()) * b_init   # scale to ±0.5\n",
    "\n",
    "# # -------- 2) connectivity seed  W0 ---------------------------------\n",
    "# #   use Pearson correlation matrix of day‑0 activity\n",
    "# X0c   = X0 - avg_rate                      # centre\n",
    "# std   = X0c.std(0, unbiased=False) + 1e-9\n",
    "# corr  = (X0c.T @ X0c) / X0c.shape[0]\n",
    "# corr  = corr / (std[:,None] * std[None,:]) # now ρ_{ij} in [‑1,1]\n",
    "# W0_init = 0.05 * corr                      # match earlier 0.05 scale\n",
    "\n",
    "# # -------- 3) low‑rank bases from new W0 ----------------------------\n",
    "# U,_,Vt = torch.linalg.svd(W0_init.cpu())\n",
    "# U = U[:,:rank]; V = Vt[:rank,:].T\n",
    "\n",
    "# print(f\"   » b range  {b_init.min():.2f} … {b_init.max():.2f}\")\n",
    "# print(f\"   » W0 diag mean±sd  {W0_init.diag().mean():.3f} ± {W0_init.diag().std():.3f}\")\n",
    "\n",
    "# # keep tensors for the model section\n",
    "# W0 = W0_init.clone()\n",
    "# B  = 0.05 * torch.randn(N,1)\n",
    "# R  = 0.05 * torch.randn(1,N)\n",
    "\n",
    "\n",
    "class SlowODE(nn.Module):\n",
    "    def __init__(self,r):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(r,64), nn.Tanh(), nn.Linear(64,r))\n",
    "    def forward(self,t,z): return self.net(z)\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.U,self.V,self.W0 = U.to(device), V.to(device), W0.to(device)\n",
    "        self.z0 = nn.Parameter(torch.zeros(rank))\n",
    "        self.slow = SlowODE(rank)\n",
    "        self.B = nn.Parameter(B.clone()); self.R = nn.Parameter(R.clone())\n",
    "        self.b_fixed = nn.Parameter(torch.zeros(N), requires_grad=True)\n",
    "        # self.b_fixed = nn.Parameter(b_init.to(device), requires_grad=False)\n",
    "\n",
    "    def weights(self,T):\n",
    "        t = torch.arange(T, dtype=torch.float32, device=self.z0.device)\n",
    "        z = odeint(self.slow, self.z0[None], t).squeeze(1)       # [T,r]\n",
    "        Ud   = self.U[None] * z[:,None]                          # [T,N,r]\n",
    "        Vt   = self.V.T.expand(T, -1, -1)                        # [T,r,N]\n",
    "        Wadd = torch.bmm(Ud, Vt)                                 # [T,N,N]\n",
    "        return self.W0[None] + Wadd\n",
    "    def rnn_cell(self,xk,vk,W):\n",
    "        return torch.tanh(W@xk + (self.B*vk).squeeze() + self.b_fixed)\n",
    "    def forward(self,x_gt,v_seq,u_seq,t_idx,Ws):\n",
    "        K = x_gt.size(0); W = Ws[t_idx]\n",
    "        rec = dec = 0.; x_prev = x_gt[0]\n",
    "        for k in range(K-1):\n",
    "            x_pred = self.rnn_cell(x_prev, v_seq[k], W)\n",
    "            rec += (x_pred - x_gt[k+1]).pow(2).sum()\n",
    "            dec += (self.R @ x_pred - u_seq[k]).pow(2)\n",
    "            x_prev = x_pred\n",
    "            # ----- inside PINN.forward  (keep all code above unchanged) -----\n",
    "            lambda_rec = 1.0      # full weight on reconstruction\n",
    "            lambda_dec = 0.1      # put decoder on same numeric scale\n",
    "\n",
    "        return lambda_rec * rec + lambda_dec * dec\n",
    "        \n",
    "\n",
    "model = PINN().to(device)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ------------------ 5. TRAIN ---------------------\n",
    "for ep in range(epochs):\n",
    "    total=0.\n",
    "    for batch in loader:\n",
    "        Ws = model.weights(T)\n",
    "        losses=[]\n",
    "        for x_gt,v_gt,u_gt,t_idx in batch:\n",
    "            x_gt,v_gt,u_gt = (x_gt.to(device), v_gt.to(device), u_gt.to(device))\n",
    "            losses.append(model(x_gt,v_gt,u_gt,t_idx,Ws))\n",
    "        rec_dec = torch.stack(losses).mean()\n",
    "        smooth  = (Ws[1:]-Ws[:-1]).pow(2).sum()\n",
    "        loss = rec_dec + LAMBDA_SMOOTH*smooth\n",
    "        opt.zero_grad(); loss.backward(); opt.step(); total+=loss.item()\n",
    "    if ep%5==0:\n",
    "        print(f\"Epoch {ep:03d} | total {total/len(loader):.3e} \"\n",
    "              f\"| rec+dec {rec_dec.item():.3e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Compute / system ===\n",
      "PyTorch: 2.8.0+cu128\n",
      "Device: cuda\n",
      "GPU: NVIDIA H200 (CUDA cc 9.0, 139.8 GB)\n",
      "CPU: x86_64\n",
      "Trainable params: 502\n",
      "\n",
      "=== Data scale ===\n",
      "Sessions: 8\n",
      "Laps per session: [12, 12, 14, 14, 12, 12, 13, 12]\n",
      "Seq length K: mean±sd = 692.6±148.5 | min/max = (419, 1521)\n",
      "Neurons N: 16\n",
      "\n",
      "=== Timing (approx) ===\n",
      "Seconds per optimization step (batch_size=1): 0.2561s\n",
      "Note: total runtime ≈ sec_per_step × (#batches per epoch) × (#epochs)\n"
     ]
    }
   ],
   "source": [
    "import time, platform, torch\n",
    "import numpy as np\n",
    "\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "def get_device_info(device):\n",
    "    if device.type == \"cuda\":\n",
    "        i = torch.cuda.current_device()\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        cap = torch.cuda.get_device_capability(i)\n",
    "        mem_gb = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        return f\"{name} (CUDA cc {cap[0]}.{cap[1]}, {mem_gb:.1f} GB)\"\n",
    "    return \"CPU\"\n",
    "\n",
    "# ---- static info ----\n",
    "print(\"=== Compute / system ===\")\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Device:\", device)\n",
    "print(\"GPU:\", get_device_info(device))\n",
    "print(\"CPU:\", platform.processor() or platform.platform())\n",
    "print(\"Trainable params:\", f\"{count_params(model):,}\")\n",
    "\n",
    "# ---- data scale ----\n",
    "num_sessions = len(x_sess)\n",
    "laps_per_day = [len(x_sess[d]) for d in range(num_sessions)]\n",
    "Ks = [x.shape[0] for d in range(num_sessions) for x in x_sess[d]]\n",
    "print(\"\\n=== Data scale ===\")\n",
    "print(\"Sessions:\", num_sessions)\n",
    "print(\"Laps per session:\", laps_per_day)\n",
    "print(\"Seq length K: mean±sd =\", f\"{np.mean(Ks):.1f}±{np.std(Ks):.1f}\",\n",
    "      \"| min/max =\", (int(np.min(Ks)), int(np.max(Ks))))\n",
    "print(\"Neurons N:\", x_sess[0][0].shape[1])\n",
    "\n",
    "# ---- quick timing: a few training steps (no full retrain) ----\n",
    "def time_steps(n_steps=20):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    it = iter(loader)\n",
    "    for _ in range(n_steps):\n",
    "        batch = next(it)\n",
    "        Ws = model.weights(T)\n",
    "        losses=[]\n",
    "        for x_gt, v_gt, u_gt, t_idx in batch:\n",
    "            x_gt, v_gt, u_gt = x_gt.to(device), v_gt.to(device), u_gt.to(device)\n",
    "            losses.append(model(x_gt, v_gt, u_gt, t_idx, Ws))\n",
    "        rec_dec = torch.stack(losses).mean()\n",
    "        smooth  = (Ws[1:]-Ws[:-1]).pow(2).sum()\n",
    "        loss = rec_dec + LAMBDA_SMOOTH*smooth\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.time() - t0\n",
    "    return dt / n_steps\n",
    "\n",
    "sec_per_step = time_steps(n_steps=20)\n",
    "print(\"\\n=== Timing (approx) ===\")\n",
    "print(f\"Seconds per optimization step (batch_size=1): {sec_per_step:.4f}s\")\n",
    "print(\"Note: total runtime ≈ sec_per_step × (#batches per epoch) × (#epochs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High neuron number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions: 8 | Neurons kept: 153/305 | Example lap shape: torch.Size([668, 153])\n",
      "‣ deriving Day‑0 statistics for W0 and b_fixed …\n",
      "Epoch 000 | total 3.247e+04 | rec+dec 2.885e+04\n",
      "Epoch 005 | total 1.529e+04 | rec+dec 1.323e+04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 172\u001b[0m\n\u001b[1;32m    170\u001b[0m     smooth  \u001b[38;5;241m=\u001b[39m (Ws[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m-\u001b[39mWs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    171\u001b[0m     loss \u001b[38;5;241m=\u001b[39m rec_dec \u001b[38;5;241m+\u001b[39m LAMBDA_SMOOTH\u001b[38;5;241m*\u001b[39msmooth\n\u001b[0;32m--> 172\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad(); loss\u001b[38;5;241m.\u001b[39mbackward(); opt\u001b[38;5;241m.\u001b[39mstep(); total\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ep\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| rec+dec \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrec_dec\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "#  DESTINODE with neuron & lap subsampling  (variable‑K, u‑decoder)\n",
    "# ===============================================================\n",
    "import numpy as np, torch, matplotlib.pyplot as plt, torch.nn as nn, random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "# ------------------- 0. KNOBS -------------------\n",
    "KEEP_NEURON_FRAC = 0.5          # 0–1   (20 % of neurons)\n",
    "KEEP_LAP_FRAC    = 0.4          # 0–1   (50 % of laps per session)\n",
    "RNG_SEED         = 42            # for reproducibility\n",
    "torch.manual_seed(RNG_SEED); np.random.seed(RNG_SEED); random.seed(RNG_SEED)\n",
    "\n",
    "# ------------------ 1. SETTINGS ------------------\n",
    "SUBSET_FILE   = \"destin_debug_subset.npz\"\n",
    "rank          = 3\n",
    "train_days    = [0, 1, 2, 3]\n",
    "test_day      = 6\n",
    "epochs        = 10\n",
    "LAMBDA_SMOOTH = 5e-2\n",
    "device        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ------------------ 2. LOAD & SUBSAMPLE ----------\n",
    "data = np.load(SUBSET_FILE, allow_pickle=True)\n",
    "sess_ids = sorted({int(k.split('_')[1]) for k in data if k.startswith('dff_')})\n",
    "\n",
    "# 2a. decide once‑and‑for‑all neuron mask\n",
    "N_full = data['dff_0'].shape[0]\n",
    "n_keep = int(np.ceil(N_full * KEEP_NEURON_FRAC))\n",
    "keep_neurons = np.random.choice(N_full, n_keep, replace=False)\n",
    "keep_neurons.sort()                                      # keep order\n",
    "\n",
    "to_tensor = lambda x: torch.tensor(x, dtype=torch.float32)\n",
    "x_sess = []; v_sess = []; u_sess = []\n",
    "for t in sess_ids:\n",
    "    dff_full = data[f'dff_{t}'][keep_neurons]             # [N_keep, K_vr]\n",
    "    dff_full = to_tensor(dff_full.T)                      # [K_vr, N_keep]\n",
    "    vel_full = to_tensor(data[f'vel_{t}'])\n",
    "    pos_full = to_tensor(data[f'pos_{t}'])\n",
    "    pos_full = (pos_full - pos_full.mean()) / pos_full.std()\n",
    "\n",
    "    lap_idx  = data[f'lap_idx_{t}']\n",
    "\n",
    "    # 2b. optional lap subsampling\n",
    "    all_laps = np.unique(lap_idx)\n",
    "    n_laps_keep = int(np.ceil(len(all_laps) * KEEP_LAP_FRAC))\n",
    "    keep_laps = np.random.choice(all_laps, n_laps_keep, replace=False)\n",
    "\n",
    "    laps_x = []; laps_v = []; laps_u = []\n",
    "    for s in sorted(keep_laps):\n",
    "        mask = lap_idx == s\n",
    "        if not mask.any(): continue\n",
    "        laps_x.append(dff_full[mask])\n",
    "        laps_v.append(vel_full[mask])\n",
    "        laps_u.append(pos_full[mask])\n",
    "\n",
    "    x_sess.append(laps_x); v_sess.append(laps_v); u_sess.append(laps_u)\n",
    "\n",
    "T, N = len(x_sess), len(keep_neurons)\n",
    "print(f\"Sessions: {T} | Neurons kept: {N}/{N_full} \"\n",
    "      f\"| Example lap shape: {x_sess[0][0].shape}\")\n",
    "\n",
    "# ------------------ 3. DATASET -------------------\n",
    "class RaggedSplitDS(Dataset):\n",
    "    def __init__(self, x, v, u, days):\n",
    "        self.x,self.v,self.u = x,v,u\n",
    "        self.map = [(t,s) for t in days for s in range(len(x[t]))]\n",
    "    def __len__(self): return len(self.map)\n",
    "    def __getitem__(self, idx):\n",
    "        t,s = self.map[idx]\n",
    "        return self.x[t][s], self.v[t][s], self.u[t][s], t\n",
    "\n",
    "loader = DataLoader(RaggedSplitDS(x_sess,v_sess,u_sess,train_days),\n",
    "                    batch_size=1, shuffle=True, collate_fn=lambda b:b)\n",
    "\n",
    "# ------------------ 4. MODEL (unchanged except N) -----------\n",
    "W0 = 0.05*torch.randn(N,N)\n",
    "U,_,Vt = torch.linalg.svd(W0)\n",
    "U = U[:,:rank]; V = Vt[:rank,:].T\n",
    "B = 0.05*torch.randn(N,1); R = 0.05*torch.randn(1,N)\n",
    "\n",
    "# ===============================================================\n",
    "#  DAY‑0‑DRIVEN INITIALISATION  (insert before “# ------------------ 4. MODEL ---------------------”)\n",
    "# ===============================================================\n",
    "print(\"‣ deriving Day‑0 statistics for W0 and b_fixed …\")\n",
    "\n",
    "# -------- gather all frames from session‑0 after subsampling --------\n",
    "X0 = torch.cat(x_sess[0], dim=0)           # [K_total_day0 , N]\n",
    "X0 = X0.float()                            # ensure fp32\n",
    "\n",
    "# -------- 1) activation bias  b_fixed -------------------------------\n",
    "avg_rate = X0.mean(0)                      # mean across time\n",
    "b_init   = -(avg_rate - avg_rate.mean())   # anti‑correlated w/ firing\n",
    "b_init   = (0.5 / b_init.abs().max()) * b_init   # scale to ±0.5\n",
    "\n",
    "# # -------- 2) connectivity seed  W0 ---------------------------------\n",
    "# #   use Pearson correlation matrix of day‑0 activity\n",
    "# X0c   = X0 - avg_rate                      # centre\n",
    "# std   = X0c.std(0, unbiased=False) + 1e-9\n",
    "# corr  = (X0c.T @ X0c) / X0c.shape[0]\n",
    "# corr  = corr / (std[:,None] * std[None,:]) # now ρ_{ij} in [‑1,1]\n",
    "# W0_init = 0.05 * corr                      # match earlier 0.05 scale\n",
    "\n",
    "# # -------- 3) low‑rank bases from new W0 ----------------------------\n",
    "# U,_,Vt = torch.linalg.svd(W0_init.cpu())\n",
    "# U = U[:,:rank]; V = Vt[:rank,:].T\n",
    "\n",
    "# print(f\"   » b range  {b_init.min():.2f} … {b_init.max():.2f}\")\n",
    "# print(f\"   » W0 diag mean±sd  {W0_init.diag().mean():.3f} ± {W0_init.diag().std():.3f}\")\n",
    "\n",
    "# # keep tensors for the model section\n",
    "# W0 = W0_init.clone()\n",
    "# B  = 0.05 * torch.randn(N,1)\n",
    "# R  = 0.05 * torch.randn(1,N)\n",
    "\n",
    "\n",
    "class SlowODE(nn.Module):\n",
    "    def __init__(self,r):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(r,64), nn.Tanh(), nn.Linear(64,r))\n",
    "    def forward(self,t,z): return self.net(z)\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.U,self.V,self.W0 = U.to(device), V.to(device), W0.to(device)\n",
    "        self.z0 = nn.Parameter(torch.zeros(rank))\n",
    "        self.slow = SlowODE(rank)\n",
    "        self.B = nn.Parameter(B.clone()); self.R = nn.Parameter(R.clone())\n",
    "        self.b_fixed = nn.Parameter(torch.zeros(N), requires_grad=True)\n",
    "        # self.b_fixed = nn.Parameter(b_init.to(device), requires_grad=False)\n",
    "\n",
    "    def weights(self,T):\n",
    "        t = torch.arange(T, dtype=torch.float32, device=self.z0.device)\n",
    "        z = odeint(self.slow, self.z0[None], t).squeeze(1)       # [T,r]\n",
    "        Ud   = self.U[None] * z[:,None]                          # [T,N,r]\n",
    "        Vt   = self.V.T.expand(T, -1, -1)                        # [T,r,N]\n",
    "        Wadd = torch.bmm(Ud, Vt)                                 # [T,N,N]\n",
    "        return self.W0[None] + Wadd\n",
    "    def rnn_cell(self,xk,vk,W):\n",
    "        return torch.tanh(W@xk + (self.B*vk).squeeze() + self.b_fixed)\n",
    "    def forward(self,x_gt,v_seq,u_seq,t_idx,Ws):\n",
    "        K = x_gt.size(0); W = Ws[t_idx]\n",
    "        rec = dec = 0.; x_prev = x_gt[0]\n",
    "        for k in range(K-1):\n",
    "            x_pred = self.rnn_cell(x_prev, v_seq[k], W)\n",
    "            rec += (x_pred - x_gt[k+1]).pow(2).sum()\n",
    "            dec += (self.R @ x_pred - u_seq[k]).pow(2)\n",
    "            x_prev = x_pred\n",
    "            # ----- inside PINN.forward  (keep all code above unchanged) -----\n",
    "            lambda_rec = 1.0      # full weight on reconstruction\n",
    "            lambda_dec = 0.1      # put decoder on same numeric scale\n",
    "\n",
    "        return lambda_rec * rec + lambda_dec * dec\n",
    "        \n",
    "\n",
    "model = PINN().to(device)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# ------------------ 5. TRAIN ---------------------\n",
    "for ep in range(epochs):\n",
    "    total=0.\n",
    "    for batch in loader:\n",
    "        Ws = model.weights(T)\n",
    "        losses=[]\n",
    "        for x_gt,v_gt,u_gt,t_idx in batch:\n",
    "            x_gt,v_gt,u_gt = (x_gt.to(device), v_gt.to(device), u_gt.to(device))\n",
    "            losses.append(model(x_gt,v_gt,u_gt,t_idx,Ws))\n",
    "        rec_dec = torch.stack(losses).mean()\n",
    "        smooth  = (Ws[1:]-Ws[:-1]).pow(2).sum()\n",
    "        loss = rec_dec + LAMBDA_SMOOTH*smooth\n",
    "        opt.zero_grad(); loss.backward(); opt.step(); total+=loss.item()\n",
    "    if ep%5==0:\n",
    "        print(f\"Epoch {ep:03d} | total {total/len(loader):.3e} \"\n",
    "              f\"| rec+dec {rec_dec.item():.3e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Compute / system ===\n",
      "PyTorch: 2.8.0+cu128\n",
      "Device: cuda\n",
      "GPU: NVIDIA H200 (CUDA cc 9.0, 139.8 GB)\n",
      "CPU: x86_64\n",
      "Trainable params: 913\n",
      "\n",
      "=== Data scale ===\n",
      "Sessions: 8\n",
      "Laps per session: [12, 12, 14, 14, 12, 12, 13, 12]\n",
      "Seq length K: mean±sd = 692.6±148.5 | min/max = (419, 1521)\n",
      "Neurons N: 153\n",
      "\n",
      "=== Timing (approx) ===\n",
      "Seconds per optimization step (batch_size=1): 0.2644s\n",
      "Note: total runtime ≈ sec_per_step × (#batches per epoch) × (#epochs)\n"
     ]
    }
   ],
   "source": [
    "import time, platform, torch\n",
    "import numpy as np\n",
    "\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "def get_device_info(device):\n",
    "    if device.type == \"cuda\":\n",
    "        i = torch.cuda.current_device()\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        cap = torch.cuda.get_device_capability(i)\n",
    "        mem_gb = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "        return f\"{name} (CUDA cc {cap[0]}.{cap[1]}, {mem_gb:.1f} GB)\"\n",
    "    return \"CPU\"\n",
    "\n",
    "# ---- static info ----\n",
    "print(\"=== Compute / system ===\")\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Device:\", device)\n",
    "print(\"GPU:\", get_device_info(device))\n",
    "print(\"CPU:\", platform.processor() or platform.platform())\n",
    "print(\"Trainable params:\", f\"{count_params(model):,}\")\n",
    "\n",
    "# ---- data scale ----\n",
    "num_sessions = len(x_sess)\n",
    "laps_per_day = [len(x_sess[d]) for d in range(num_sessions)]\n",
    "Ks = [x.shape[0] for d in range(num_sessions) for x in x_sess[d]]\n",
    "print(\"\\n=== Data scale ===\")\n",
    "print(\"Sessions:\", num_sessions)\n",
    "print(\"Laps per session:\", laps_per_day)\n",
    "print(\"Seq length K: mean±sd =\", f\"{np.mean(Ks):.1f}±{np.std(Ks):.1f}\",\n",
    "      \"| min/max =\", (int(np.min(Ks)), int(np.max(Ks))))\n",
    "print(\"Neurons N:\", x_sess[0][0].shape[1])\n",
    "\n",
    "# ---- quick timing: a few training steps (no full retrain) ----\n",
    "def time_steps(n_steps=20):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    it = iter(loader)\n",
    "    for _ in range(n_steps):\n",
    "        batch = next(it)\n",
    "        Ws = model.weights(T)\n",
    "        losses=[]\n",
    "        for x_gt, v_gt, u_gt, t_idx in batch:\n",
    "            x_gt, v_gt, u_gt = x_gt.to(device), v_gt.to(device), u_gt.to(device)\n",
    "            losses.append(model(x_gt, v_gt, u_gt, t_idx, Ws))\n",
    "        rec_dec = torch.stack(losses).mean()\n",
    "        smooth  = (Ws[1:]-Ws[:-1]).pow(2).sum()\n",
    "        loss = rec_dec + LAMBDA_SMOOTH*smooth\n",
    "        loss.backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    dt = time.time() - t0\n",
    "    return dt / n_steps\n",
    "\n",
    "sec_per_step = time_steps(n_steps=20)\n",
    "print(\"\\n=== Timing (approx) ===\")\n",
    "print(f\"Seconds per optimization step (batch_size=1): {sec_per_step:.4f}s\")\n",
    "print(\"Note: total runtime ≈ sec_per_step × (#batches per epoch) × (#epochs)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
